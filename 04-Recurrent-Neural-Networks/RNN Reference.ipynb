{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/datadriveninvestor/multivariate-time-series-using-rnn-with-keras-7f78f4488679\n",
    "    \n",
    "https://medium.com/datadriveninvestor/recurrent-neural-network-rnn-52dd4f01b7e8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need to work on **_sequential_** data that needs to be persisted over several time steps then we use Recurrent Neural network(RNN).\n",
    "\n",
    "Traditional neural network and CNN’s need a fixed input vector, apply activation function on fixed set of layers to produce a fix sized output. <br>\n",
    "\n",
    "For example we take an input image of 128 by 128 sized vector to predict images of dogs or cats or cars. we cannot take a variable sized image to make the prediction.\n",
    "\n",
    "Now what if we need to operate over sequential data that is dependent on previous input state like our message or when sequential data can be in input or output or both, and that is exactly where we use RNNs.\n",
    "\n",
    "In RNN, we share the weights and feed the output back into the inputs recursively.This recurrent formulation helps process sequential data.\n",
    "\n",
    "**RNN’s make use of sequential data to make inferences** like who is talking, what is being spoken and what might be the next word etc.\n",
    "\n",
    "**RNN’s are neural networks with loops to persist information.** RNN are called a **recurrent as they perform the same task for every element in the sequence and output elements are dependent on previous elements or states**. This is is how RNN’s persist information to use context to draw inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='RNN.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is RNN Used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN’s are used for\n",
    "- Image classification\n",
    "- Image captioning\n",
    "- Machine Translation\n",
    "- Video classification\n",
    "- Sentiment Analysis\n",
    "\n",
    "<img src='what_is_rnn_used_for.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does RNN Work\n",
    "\n",
    "- h is hidden state\n",
    "- x is input\n",
    "- y is output\n",
    "- W is the Weights\n",
    "- t is time step\n",
    "\n",
    "As we are processing sequential data, **RNN takes an input x at a time step t. RNN takes the hidden state value at time step t-1 to calculate the hidden state h at time step t and applying a tanh activation function. we use tanh or ReLU for non linearity in the output y at time time t.**\n",
    "\n",
    "<img src='how_does_RNN_work.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike other deep neural networks which uses a different parameter for each hidden layer, **RNN shares the same weight parameter at each step.**\n",
    "\n",
    "we randomly initialize the weight matrices, and during the training we need to find the values of the matrices that give us desirable behaviour, so we calculate the loss function L. Loss function L is calculated by measuring difference between actual output and the predicted output.. To calculate L, we use Cross Entropy function.\n",
    "\n",
    "<img src='RNN_loss.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the loss, we use back propagation but unlike traditional neural nets, RNN’s share weights across multiple layers or in other words it shares weight across all the time steps. This way the gradient of error at each step is also dependent on the loss at previous steps.\n",
    "\n",
    "In the above example to calculate the gradient at step 4, we need to sum up the losses of the previous 3 steps along with the loss at the fourth time step. This is called **Backpropagation through Time -BPPT.**\n",
    "\n",
    "We calculate gradient of the error with respect to weights for us to learn the right weights for us get a desirable output.\n",
    "As W is used in every step till the output we care about, we back propagate from t=4 to t=0. In traditional neural net we do not share weight so we do not need to sum the gradients however **in RNN we share weights and we need to sum up the gradients for W at each step in time.**\n",
    "\n",
    "Computing gradient of h at time step t =0 involves many factors of W as we need to back propagate through each of the RNN cells. Even if we forget the weight matrix and multiply the same scalar value again and again for let’s say 100 time steps and this will can be a challenge.\n",
    "\n",
    "If the largest singular value is greater than 1 then the gradient would explode, called as **Exploding gradient.**\n",
    "\n",
    "If the largest singular value is less than 1 then the gradient would vanish, called as **Vanishing gradient.**\n",
    "\n",
    "<img src='RNN_forward_backforward.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use Gradient Clipping for exploding gradient problem where we can pre set a threshold value and if the gradient value is greater than the threshold value we can clip it.\n",
    "\n",
    "For solving vanishing gradient popular way it to use **LSTM(Long Short Term Memory)** or **Gated Recurrent Unit(GRU).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='LSTM.png'>\n",
    "\n",
    "<img src='LSTM_3steps.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First step in LSTM is to decide if we need to remember or forget the cell state. Forget gate uses sigmoid activation function which gives an output value of 0 or 1. An output of 1 from forget gate tells us that we want to keep the value and a value of 0 tells us that we want to forget the value.\n",
    "\n",
    "2. Second step to decide what new information will we store in the cell state. This has two parts one is the input gate, which decide whether to write to the cell state using the sigmoid function and the gated gate, which decides how much to write to the cell state using tanh activation function. Gated gate creates a vector or new candidate values that can be added to the cell state\n",
    "\n",
    "3. In the last step we create the cell state by combining the outputs from step 1 and step2 which is multiplying the cell state after applying a tanh activation function for the current time step by the output gate’s output. Tanh activation function gives and output range between -1 and +1\n",
    "\n",
    "4. Cell state which is the internal memory of the unit combines the previous cell state multiplied by the forget gate and then new computed hidden state in g, multiplied by the input gate i’s output.\n",
    "\n",
    "Finally the output will be based on the cell satte but will be a filtered version.\n",
    "\n",
    "<img src='LSTM_example.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU — a variant of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU uses two gates reset gate and an update gate unlike the three steps in LSTM. GRU does not have an internal memory\n",
    "Reset gate decides how to combine the new input with the previous time steps’s memory.\n",
    "\n",
    "Update gate decides how much of the previous memory should be kept. Update gate is a combination of input and forget gate that we understood in LSTM.\n",
    "GRU is simpler variant of LSTM to solve vanishing gradient problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
